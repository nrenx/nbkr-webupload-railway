# NBKRIST Student Portal - Web Scraper

This project provides a web interface for running web scraping scripts to collect data from the NBKRIST student portal and upload it to Supabase Storage.

## Features

- Web interface for running scraper scripts
- Headless mode enabled by default for all scripts
- Sequential execution of multiple scripts
- Job management system with progress tracking
- Direct upload to Supabase Storage
- Optimized for Render's free tier

## Deployment on Render

### Prerequisites

1. A GitHub account
2. A Render account (free tier is sufficient)
3. A Supabase account with a project set up

### Steps to Deploy

1. Push this code to a GitHub repository
2. Log in to your Render account
3. Click "New" and select "Web Service"
4. Connect your GitHub repository
5. Use the following settings:
   - Name: `nbkrist-student-portal` (or any name you prefer)
   - Environment: `Python`
   - Build Command: `pip install -r requirements.txt`
   - Start Command: `gunicorn app:app`
   - Instance Type: `Free`

6. Click "Create Web Service"

## Usage

1. Access your deployed application at the URL provided by Render
2. Enter login credentials and select the academic year
3. Select the scripts you want to run
4. Click "Run Selected Scripts"
5. Monitor the progress on the status page

Note: The interface has been simplified to remove Semester, Branch, Section, and Test Script options for better usability.

## Scripts

### Attendance Scraper

Scrapes attendance data for students based on the selected academic details.

### Mid Marks Scraper

Scrapes mid-term marks data for students based on the selected academic details.

### Personal Details Scraper

Scrapes personal details for students based on the selected academic details.

### Direct Supabase Uploader

Uploads the scraped data directly to Supabase Storage.

## Render Free Tier Considerations

This application is optimized for Render's free tier with the following considerations:

1. **Ephemeral Storage**: All data is stored in Render's temporary storage (`/tmp` directory) during script execution and then uploaded to Supabase.

2. **Spin Down on Idle**: The application will spin down after 15 minutes of inactivity and spin back up when a new request is received.

3. **Headless Mode**: All web scraping is done in headless mode to minimize resource usage.

4. **Sequential Execution**: Scripts run one after another to optimize performance and stay within resource limits.

5. **Job Management**: Only one job runs at a time to prevent resource contention.

6. **Improved Authentication Handling**: Authentication failures are detected early with clear error messages, preventing excessive retries and resource usage.

7. **Simplified Interface**: The web interface has been streamlined to focus on essential options, making it more user-friendly and efficient.

## Configuration

### Supabase Configuration

Create a `supabase_config.py` file with your Supabase credentials:

```python
# Supabase credentials
SUPABASE_URL = "your-supabase-url"
SUPABASE_KEY = "your-supabase-key"

# Default settings
DEFAULT_SETTINGS = {
    # Storage settings
    "bucket": "student_data",
    "source_dir": "/tmp/student_details",

    # Performance settings
    "workers": 32,
    "student_batch": 20,

    # Feature settings
    "skip_existing": True,
}
```

### Environment Variables

The following environment variables can be set in the Render dashboard:

- `SECRET_KEY`: Secret key for Flask session encryption (automatically generated by Render)
- `SUPABASE_URL`: Your Supabase URL
- `SUPABASE_KEY`: Your Supabase API key
- `SUPABASE_BUCKET`: The name of your Supabase storage bucket (default: "student-details")

## Development

### Local Setup

1. Clone the repository
2. Install dependencies: `pip install -r requirements.txt`
3. Run the application: `python app.py`
4. Access the application at `http://localhost:8000`

### File Structure

- `app.py`: Main Flask application
- `taskmaster.py`: Job management system
- `direct_supabase_uploader.py`: Direct upload to Supabase
- `attendance_scraper.py`: Attendance data scraper
- `mid_marks_scraper.py`: Mid-term marks data scraper
- `personal_details_scraper.py`: Personal details scraper
- `config.py`: Configuration settings
- `supabase_config.py`: Supabase credentials and settings
- `templates/`: HTML templates for the web interface
- `render.yaml`: Render deployment configuration

## License

This project is licensed under the MIT License - see the LICENSE file for details.
